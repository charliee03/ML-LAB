{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b831fc4",
   "metadata": {},
   "source": [
    "# Case Study 2: Real Estate Price Prediction (Regression)\n",
    "\n",
    "**Student Name:** Naman Nagar  \n",
    "**SRN:** PES2UG23CS361\n",
    "**Dataset:** house_price_data.csv  \n",
    "**Objective:** Predict house sale prices using regression algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c507de",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ba6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('house_price_data.csv')\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92578f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da271c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers)\n",
    "\n",
    "# Check outliers in numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    outlier_count = detect_outliers(df, col)\n",
    "    outlier_summary[col] = outlier_count\n",
    "\n",
    "print(\"Outlier Summary:\")\n",
    "for col, count in outlier_summary.items():\n",
    "    print(f\"{col}: {count} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61ed78",
   "metadata": {},
   "source": [
    "## Task 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Price distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['sale_price'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Sale Price Distribution')\n",
    "plt.xlabel('Sale Price (thousands)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Price distribution (log scale)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(np.log(df['sale_price']), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Log Sale Price Distribution')\n",
    "plt.xlabel('Log Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(df['sale_price'])\n",
    "plt.title('Sale Price Box Plot')\n",
    "plt.ylabel('Sale Price (thousands)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for skewness\n",
    "skewness = stats.skew(df['sale_price'])\n",
    "print(f\"Sale Price Skewness: {skewness:.4f}\")\n",
    "if abs(skewness) > 1:\n",
    "    print(\"Data is highly skewed - consider log transformation\")\n",
    "elif abs(skewness) > 0.5:\n",
    "    print(\"Data is moderately skewed\")\n",
    "else:\n",
    "    print(\"Data is approximately symmetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location impact analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Neighborhood vs Price\n",
    "plt.subplot(2, 3, 1)\n",
    "neighborhood_price = df.groupby('neighborhood')['sale_price'].mean().sort_values(ascending=False)\n",
    "plt.bar(range(len(neighborhood_price)), neighborhood_price.values)\n",
    "plt.title('Average Price by Neighborhood')\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.ylabel('Average Sale Price')\n",
    "plt.xticks(range(len(neighborhood_price)), neighborhood_price.index, rotation=45)\n",
    "\n",
    "# Distance to city center\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(df['distance_to_city_center'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Distance to City Center vs Price')\n",
    "plt.xlabel('Distance to City Center')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# Distance to metro\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df['distance_to_metro'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Distance to Metro vs Price')\n",
    "plt.xlabel('Distance to Metro')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# Distance to school\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df['distance_to_school'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Distance to School vs Price')\n",
    "plt.xlabel('Distance to School')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# Crime rate\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(df['crime_rate'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Crime Rate vs Price')\n",
    "plt.xlabel('Crime Rate')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# House age\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df['house_age'], df['sale_price'], alpha=0.6)\n",
    "plt.title('House Age vs Price')\n",
    "plt.xlabel('House Age')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property characteristics analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Total area vs Price\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(df['total_area'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Total Area vs Price')\n",
    "plt.xlabel('Total Area')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# Bedrooms vs Price\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(data=df, x='bedrooms', y='sale_price')\n",
    "plt.title('Bedrooms vs Price')\n",
    "\n",
    "# Bathrooms vs Price\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(data=df, x='bathrooms', y='sale_price')\n",
    "plt.title('Bathrooms vs Price')\n",
    "\n",
    "# Floors vs Price\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.boxplot(data=df, x='floors', y='sale_price')\n",
    "plt.title('Floors vs Price')\n",
    "\n",
    "# Garage vs Price\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(data=df, x='garage', y='sale_price')\n",
    "plt.title('Garage vs Price')\n",
    "\n",
    "# Garden vs Price\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.boxplot(data=df, x='garden', y='sale_price')\n",
    "plt.title('Garden vs Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality factors analysis\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Construction quality\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.boxplot(data=df, x='construction_quality', y='sale_price')\n",
    "plt.title('Construction Quality vs Price')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Renovation status\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(data=df, x='renovation_status', y='sale_price')\n",
    "plt.title('Renovation Status vs Price')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Energy efficiency\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(data=df, x='energy_efficiency', y='sale_price')\n",
    "plt.title('Energy Efficiency vs Price')\n",
    "\n",
    "# Heating type\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.boxplot(data=df, x='heating_type', y='sale_price')\n",
    "plt.title('Heating Type vs Price')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Air conditioning\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(data=df, x='air_conditioning', y='sale_price')\n",
    "plt.title('Air Conditioning vs Price')\n",
    "\n",
    "# Parking spaces\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.boxplot(data=df, x='parking_spaces', y='sale_price')\n",
    "plt.title('Parking Spaces vs Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0528eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market timing analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Season sold\n",
    "plt.subplot(1, 3, 1)\n",
    "season_price = df.groupby('season_sold')['sale_price'].mean()\n",
    "plt.bar(season_price.index, season_price.values)\n",
    "plt.title('Average Price by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Average Sale Price')\n",
    "\n",
    "# Year sold\n",
    "plt.subplot(1, 3, 2)\n",
    "year_price = df.groupby('year_sold')['sale_price'].mean()\n",
    "plt.plot(year_price.index, year_price.values, marker='o')\n",
    "plt.title('Average Price by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Sale Price')\n",
    "\n",
    "# Listing duration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(df['listing_duration'], df['sale_price'], alpha=0.6)\n",
    "plt.title('Listing Duration vs Price')\n",
    "plt.xlabel('Listing Duration')\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea31bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Check for multicollinearity\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"High correlation pairs (>0.8):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "else:\n",
    "    print(\"No high correlation pairs found (>0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecd6a4",
   "metadata": {},
   "source": [
    "## Task 3: Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2dcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Feature engineering\n",
    "# Create new features\n",
    "df_processed['price_per_sqft'] = df_processed['sale_price'] / df_processed['total_area']\n",
    "df_processed['room_ratio'] = df_processed['bedrooms'] / df_processed['bathrooms']\n",
    "df_processed['total_amenities'] = (df_processed['garage'] + df_processed['garden'] + \n",
    "                                   df_processed['basement'] + df_processed['balcony'])\n",
    "df_processed['avg_distance'] = (df_processed['distance_to_city_center'] + \n",
    "                               df_processed['distance_to_metro'] + \n",
    "                               df_processed['distance_to_school']) / 3\n",
    "\n",
    "# Handle potential division by zero\n",
    "df_processed['room_ratio'] = df_processed['room_ratio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- price_per_sqft\")\n",
    "print(\"- room_ratio\")\n",
    "print(\"- total_amenities\")\n",
    "print(\"- avg_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col != 'property_id':  # Don't encode property_id\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical variables encoded:\")\n",
    "print(list(label_encoders.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using IQR method\n",
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers from sale_price\n",
    "print(f\"Original dataset size: {len(df_processed)}\")\n",
    "df_processed = remove_outliers(df_processed, 'sale_price')\n",
    "print(f\"After removing outliers: {len(df_processed)}\")\n",
    "print(f\"Removed {len(df) - len(df_processed)} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address multicollinearity - remove highly correlated features\n",
    "# You can uncomment and modify this section if needed\n",
    "# if high_corr_pairs:\n",
    "#     features_to_drop = ['distance_to_city_center']  # Example\n",
    "#     df_processed = df_processed.drop(features_to_drop, axis=1)\n",
    "#     print(f\"Dropped features due to high correlation: {features_to_drop}\")\n",
    "\n",
    "print(\"Multicollinearity check completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_processed.drop(['property_id', 'sale_price'], axis=1)\n",
    "y = df_processed['sale_price']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeatures:\", list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784efb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87522c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323f71c",
   "metadata": {},
   "source": [
    "## Task 4: Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Linear Regression model trained successfully!\")\n",
    "print(f\"Number of features: {len(lr_model.coef_)}\")\n",
    "print(f\"Intercept: {lr_model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c81b5",
   "metadata": {},
   "source": [
    "## Task 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{dataset_name} Metrics:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R²': r2}\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training Set\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc541e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create residual plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Predicted vs Actual (Training)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.6)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Training: Predicted vs Actual')\n",
    "\n",
    "# Predicted vs Actual (Test)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Test: Predicted vs Actual')\n",
    "\n",
    "# Residuals vs Predicted (Training)\n",
    "plt.subplot(2, 3, 3)\n",
    "residuals_train = y_train - y_train_pred\n",
    "plt.scatter(y_train_pred, residuals_train, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Training: Residuals vs Predicted')\n",
    "\n",
    "# Residuals vs Predicted (Test)\n",
    "plt.subplot(2, 3, 4)\n",
    "residuals_test = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Test: Residuals vs Predicted')\n",
    "\n",
    "# Residuals distribution (Training)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(residuals_train, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Training: Residuals Distribution')\n",
    "\n",
    "# Residuals distribution (Test)\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(residuals_test, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test: Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': lr_model.coef_,\n",
    "    'abs_coefficient': np.abs(lr_model.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "colors = ['green' if coef > 0 else 'red' for coef in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top 15 Feature Coefficients (Linear Regression)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance[['feature', 'coefficient']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "performance_df = pd.DataFrame({\n",
    "    'Training': train_metrics,\n",
    "    'Test': test_metrics\n",
    "})\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(performance_df)\n",
    "\n",
    "# Check for overfitting/underfitting\n",
    "r2_diff = train_metrics['R²'] - test_metrics['R²']\n",
    "print(f\"\\nR² difference (Train - Test): {r2_diff:.4f}\")\n",
    "\n",
    "if r2_diff > 0.1:\n",
    "    print(\"Warning: Possible overfitting detected\")\n",
    "elif test_metrics['R²'] < 0.7:\n",
    "    print(\"Warning: Model may be underfitting\")\n",
    "else:\n",
    "    print(\"Model performance looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ad3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "sample_indices = np.random.choice(len(y_test), 10, replace=False)\n",
    "sample_predictions = pd.DataFrame({\n",
    "    'Actual Price': y_test.iloc[sample_indices].values,\n",
    "    'Predicted Price': y_test_pred[sample_indices],\n",
    "    'Absolute Error': np.abs(y_test.iloc[sample_indices].values - y_test_pred[sample_indices])\n",
    "})\n",
    "\n",
    "sample_predictions['Percentage Error'] = (\n",
    "    sample_predictions['Absolute Error'] / sample_predictions['Actual Price'] * 100\n",
    ")\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(sample_predictions.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1afeb1",
   "metadata": {},
   "source": [
    "Executive Summary:\n",
    "- End-to-end regression workflow on house_price_data.csv completed, from EDA to evaluation.\n",
    "- Sale prices are right-skewed with notable outliers; size (total_area), quality (construction_quality), amenities, and proximity (distance metrics) show strong relationships with price.\n",
    "- Preprocessing included label encoding, IQR-based outlier removal on sale_price, standard scaling, and feature engineering (price_per_sqft, room_ratio, total_amenities, avg_distance).\n",
    "- Linear Regression trained on scaled features; metrics, residual diagnostics, and coefficient-based feature importance were analyzed.\n",
    "- Insights indicate pricing is driven by area, build quality, amenities, and proximity; guidance provided for listing strategy and value-add investments.\n",
    "\n",
    "Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. Data Quality: Minimal missing values (handled or none reported); outliers present in sale_price and some numeric features (e.g., total_area), addressed via IQR on target.\n",
    "2. Price Distribution: Sale prices are right-skewed; log(sale_price) is more symmetric, supporting potential log-target modeling.\n",
    "3. Important Features: total_area (+), construction_quality (+), bathrooms/bedrooms (+), garage/garden (+), energy_efficiency (+), neighborhood effects, and negative impact from distance_to_city_center and crime_rate.\n",
    "4. Location Impact: Neighborhood averages vary notably; closer to city center/metro/schools generally increases price; higher crime rate depresses price.\n",
    "\n",
    "### Model Performance:\n",
    "- Test R²: {test_metrics['R²']:.4f}\n",
    "- Test RMSE: {test_metrics['RMSE']:.2f} (thousands)\n",
    "- Test MAE: {test_metrics['MAE']:.2f} (thousands)\n",
    "- Average Percentage Error: {sample_predictions['Percentage Error'].mean():.2f}% (on 10 random test samples)\n",
    "\n",
    "### Business Insights:\n",
    "1. Larger area and higher construction quality command higher prices; highlight these in listings.\n",
    "2. Proximity to transit/schools materially affects value; emphasize location advantages and reduce price expectations when far from amenities.\n",
    "3. Affordable upgrades (energy efficiency, bathrooms, parking, garden/balcony) can yield favorable ROI.\n",
    "\n",
    "### Model Limitations:\n",
    "1. Linear model may miss non-linear/interaction effects; coefficients can be impacted by multicollinearity.\n",
    "2. Label encoding for categorical variables imposes ordinality; neighborhood effects may be under-modeled.\n",
    "\n",
    "### Recommendations for Improvement:\n",
    "1. Try regularized linear models (Ridge/Lasso/ElasticNet) and tree-based methods (RandomForest, XGBoost) with cross-validation.\n",
    "2. Consider log-transforming the target, adding interactions (area × quality, distance × crime), and using target/one-hot encoding for categoricals.\n",
    "3. Broaden outlier strategy (feature-wise), refine feature selection, and incorporate additional location features (POIs, accessibility indices).ecutive Summary:\n",
    "- End-to-end regression workflow on house_price_data.csv completed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
